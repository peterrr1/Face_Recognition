{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import shufflenet_v2_x0_5, ShuffleNet_V2_X0_5_Weights, mobilenet_v2, efficientnet_b0\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch import nn\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "import mlflow\n",
    "from pprint import pprint\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## Add the path to the utils folder\n",
    "sys.path.append(str(Path('../src/python/local').resolve()))\n",
    "from utils.transform_utils import ShuffleNet_V2_X0_5_FaceTransforms\n",
    "from utils.constant_utils import celeba_columns\n",
    "from utils.metric_utils import evaluate_performance\n",
    "from datasets.CelebA import CelebA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('http://localhost:8080')\n",
    "mlflow.set_experiment('logger_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=1024, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=False),\n",
    "    nn.Linear(in_features=1000, out_features=40, bias=True),\n",
    ")\n",
    "\n",
    "classifier[1].in_features = shufflenet_v2_x0_5().fc.in_features\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_classifier(model, classifier):\n",
    "    ## Change the classifier\n",
    "    if model.__class__.__name__ == 'ShuffleNetV2':\n",
    "        model.fc = classifier\n",
    "    else:\n",
    "        model.classifier = classifier\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    ## Freeze all the parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    ## Unfreeze the last layer\n",
    "    if model.__class__.__name__ == 'ShuffleNetV2':\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1024, out_features=1000, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShuffleNetV2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1000, out_features=40, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(shufflenet_v2_x0_5().fc)\n",
    "model = change_classifier(shufflenet_v2_x0_5(), classifier)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=False)\n",
      "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=1000, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mobilenet_v2().classifier)\n",
    "model = change_classifier(mobilenet_v2(), classifier)\n",
    "model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_b0().__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_metrics():\n",
    "    zero_metrics = {\n",
    "        'averaged_example_based_accuracy': 0.0,\n",
    "        'f1_score': {\n",
    "            'macro_averaged': 0.0,\n",
    "            'micro_averaged': 0.0,\n",
    "            'per_label': np.zeros(40, dtype=float),  # Assuming the array has 40 elements\n",
    "            'sample_average': 0.0,\n",
    "            'weighted_averaged': 0.0\n",
    "        },\n",
    "        'hamming_loss': 0.0,\n",
    "        'precision_score': {\n",
    "            'macro_averaged': 0.0,\n",
    "            'micro_averaged': 0.0,\n",
    "            'per_label': np.zeros(40, dtype=float),\n",
    "            'sample_average': 0.0,\n",
    "            'weighted_averaged': 0.0\n",
    "        },\n",
    "        'recall_score': {\n",
    "            'macro_averaged': 0.0,\n",
    "            'micro_averaged': 0.0,\n",
    "            'per_label': np.zeros(40, dtype=float),\n",
    "            'sample_average': 0.0,\n",
    "            'weighted_averaged': 0.0\n",
    "        }\n",
    "    }\n",
    "    return zero_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_dict(dict):\n",
    "    for key, value in dict.items():\n",
    "        if isinstance(value, Mapping):\n",
    "            print(key)\n",
    "            traverse_dict(value)\n",
    "        else:\n",
    "            print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = create_zero_metrics()\n",
    "traverse_dict(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model and detector\n",
    "detector = YOLO(\"../src/python/static/yolov11n-face.pt\")\n",
    "model = shufflenet_v2_x0_5(weights=ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(in_features=1024, out_features=40, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('./downloaded_ds/000011.jpg')\n",
    "image = ShuffleNet_V2_X0_5_FaceTransforms(detector, pad=15)(image)\n",
    "plt.imshow(v2.ToPILImage()(image))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = detector.predict(image.unsqueeze(0))\n",
    "face[0].boxes.conf.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 64\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "params = {\n",
    "    'epochs': epochs,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': batch_size,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'loss': criterion.__class__.__name__\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba = CelebA('../data/celeba', transform=ShuffleNet_V2_X0_5_FaceTransforms(detector, pad=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(celeba.attr.shape)\n",
    "print(celeba.files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, _, demo = torch.utils.data.random_split(celeba, [0.7, 0.298, 0.002], torch.Generator().manual_seed(0))\n",
    "train, val, test = torch.utils.data.random_split(demo, [0.7, 0.2, 0.1], torch.Generator().manual_seed(0))\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size)\n",
    "test_loader = DataLoader(test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "num_of_pos_labels = torch.sum(dataset.labels, dim = 0)\n",
    "num_of_neg_labels = num_of_labels - num_of_pos_labels\n",
    "\n",
    "pos_weights = num_of_neg_labels / num_of_pos_labels\n",
    "\n",
    "obj_pos_weights = pos_weights.index_select(dim=0, index=objective_label_idx)\n",
    "subj_pos_weights = pos_weights.index_select(dim=0, index=subjective_label_idx)\n",
    "\"\"\"\n",
    "num_of_labels = len(celeba.attr)\n",
    "num_of_pos_labels = torch.sum(celeba.attr, dim = 0)\n",
    "num_of_neg_labels = num_of_labels - num_of_pos_labels\n",
    "pos_weights = num_of_neg_labels / num_of_pos_labels\n",
    "pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair',\n",
    "          'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby','Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup',\n",
    "          'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose',\n",
    "          'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
    "          'Wearing_Necklace', 'Wearing_Necktie', 'Young']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CalculateMetric api class\n",
    "*    calculates all of the metrics and returns the result as json\n",
    "*    does not handle mlflow logging\n",
    "\n",
    "#### MLFlowLogger api class\n",
    "* when invoked gets the metrics from the CM class api as json\n",
    "* processes the json and then logs the metrics to the mlflow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_metrics():\n",
    "    zero_metrics = {\n",
    "        'averaged_example_based_accuracy': 0.0,\n",
    "        'f1_score': {\n",
    "            'macro_averaged': 0.0,\n",
    "            'micro_averaged': 0.0,\n",
    "            'per_label': np.zeros(40, dtype=float),  # Assuming the array has 40 elements\n",
    "            'sample_average': 0.0,\n",
    "            'weighted_averaged': 0.0\n",
    "        },\n",
    "        'hamming_loss': 0.0,\n",
    "        'precision_score': {\n",
    "            'macro_averaged': 0.0,\n",
    "            'micro_averaged': 0.0,\n",
    "            'per_label': np.zeros(40, dtype=float),\n",
    "            'sample_average': 0.0,\n",
    "            'weighted_averaged': 0.0\n",
    "        },\n",
    "        'recall_score': {\n",
    "            'macro_averaged': 0.0,\n",
    "            'micro_averaged': 0.0,\n",
    "            'per_label': np.zeros(40, dtype=float),\n",
    "            'sample_average': 0.0,\n",
    "            'weighted_averaged': 0.0\n",
    "        }\n",
    "    }\n",
    "    return zero_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsLogger():\n",
    "    def __init__(self, columns):\n",
    "        self.df_f1 = pd.DataFrame(columns=columns)\n",
    "        self.df_rec = pd.DataFrame(columns=columns)\n",
    "        self.df_prec = pd.DataFrame(columns=columns)\n",
    "        \n",
    "    \n",
    "    def log_metrics(self, stage, metrics, step):\n",
    "        \n",
    "        mlflow.log_metric(f\"Loss_{stage}\", f\"{metrics['loss']:2f}\", step=step)\n",
    "        mlflow.log_metric(f\"Hamming_loss_{stage}\", f\"{metrics['hamming_loss']:2f}\", step=step)\n",
    "        mlflow.log_metric(f'Subset_Accuracy_{stage}', metrics['averaged_example_based_accuracy'], step=step)\n",
    "\n",
    "        mlflow.log_metric(f'F1_Score_Micro_{stage}', metrics['f1_score']['micro_averaged'], step=step)\n",
    "        mlflow.log_metric(f'F1_Score_Macro_{stage}', metrics['f1_score']['micro_averaged'], step=step)\n",
    "        mlflow.log_metric(f'F1_Score_Sample_{stage}', metrics['f1_score']['sample_average'], step=step)\n",
    "        mlflow.log_metric(f'F1_Score_Weighted_{stage}', metrics['f1_score']['weighted_averaged'], step=step)\n",
    "\n",
    "        mlflow.log_metric(f'Recall_Micro_{stage}', metrics['recall_score']['micro_averaged'], step=step)\n",
    "        mlflow.log_metric(f'Recall_Macro_{stage}', metrics['recall_score']['macro_averaged'], step=step)\n",
    "        mlflow.log_metric(f'Recall_Sample_{stage}', metrics['recall_score']['sample_average'], step=step)\n",
    "        mlflow.log_metric(f'Recall_Weighted_{stage}', metrics['recall_score']['weighted_averaged'], step=step)\n",
    "\n",
    "        mlflow.log_metric(f'Precision_Micro_{stage}', metrics['precision_score']['micro_averaged'], step=step)\n",
    "        mlflow.log_metric(f'Precision_Macro_{stage}', metrics['precision_score']['macro_averaged'], step=step)\n",
    "        mlflow.log_metric(f'Precision_Sample_{stage}', metrics['precision_score']['sample_average'], step=step)\n",
    "        mlflow.log_metric(f'Precision_Weighted_{stage}', metrics['precision_score']['weighted_averaged'], step=step)\n",
    "\n",
    "        \"\"\"\n",
    "        self.df_f1.loc[step] = metrics['f1_score']['per_label']\n",
    "        self.df_rec.loc[step] = metrics['recall_score']['per_label']\n",
    "        self.df_prec.loc[step] = metrics['precision_score']['per_label']\n",
    "\n",
    "        print(self.df_f1.loc[step])\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def save_artifact(self, path):\n",
    "        mlflow.log_table(self.df_f1, 'f1_score_per_label.json')\n",
    "        mlflow.log_table(self.df_rec, 'recall_per_label.json')\n",
    "        mlflow.log_table(self.df_prec, 'precision_per_label.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dicts(dict1, dict2):\n",
    "    \"\"\"Recursively adds two dictionaries with the same structure.\"\"\"\n",
    "    if not dict1 or not dict2:\n",
    "        return dict1 or dict2  # Return whichever is not empty\n",
    "\n",
    "    result = {}\n",
    "    for key in dict1:\n",
    "        if isinstance(dict1[key], Mapping):  # If it's a nested dictionary, recurse\n",
    "            result[key] = add_dicts(dict1[key], dict2[key])\n",
    "        elif isinstance(dict1[key], np.ndarray):  # Element-wise sum for NumPy arrays\n",
    "            result[key] = dict1[key] + dict2[key]\n",
    "        else:  # Normal scalar sum\n",
    "            result[key] = dict1[key] + dict2[key]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def divide_dict(d, divisor):\n",
    "    if divisor == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    \n",
    "    if isinstance(d, dict):\n",
    "        return {k: divide_dict(v, divisor) for k, v in d.items()}\n",
    "    elif isinstance(d, np.ndarray):\n",
    "        return d / divisor\n",
    "    elif isinstance(d, (int, float)):\n",
    "        return d / divisor\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "def print_metrics(epoch, loss_sum, metrics_sum, avg_metrics, avg_loss):\n",
    "    print(f'Epoch {epoch + 1} - Summed Loss: {loss_sum}')\n",
    "    print(f'Epoch {epoch + 1} - Average Loss: {avg_loss}')\n",
    "\n",
    "    print(f'Summed metrics in epoch {epoch + 1}:')\n",
    "    pprint(metrics_sum)\n",
    "\n",
    "    print(f'Avareged metrics in epoch {epoch + 1}:')\n",
    "    pprint(avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.types import Schema, TensorSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "\n",
    "input_schema = Schema([TensorSpec(np.dtype(np.float32), (1, 3, 224, 224))])\n",
    "output_schema = Schema([TensorSpec(np.dtype(np.float32), (1, 40))])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='TrainDemo') as run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.set_tag('Training info', 'ShuffleNetV2_X0_5')\n",
    "        mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(model)))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_model = f\"runs:/{run.info.run_id}/model\"\n",
    "loaded_model = mlflow.pytorch.load_model(logged_model)\n",
    "loaded_model(torch.rand(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader,\n",
    "        'test': test_loader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, epochs, logger):\n",
    "    model.train()\n",
    "\n",
    "    with mlflow.start_run(run_name='TrainDemo'):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.set_tag('Training info', 'ShuffleNetV2_X0_5')\n",
    "        mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(model)))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "        mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss_sum = 0.0\n",
    "            metrics_sum = create_zero_metrics()\n",
    "\n",
    "            for batch, (input, target) in enumerate(loader['train']):\n",
    "                input = input.to(device)\n",
    "                target = target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = model(input)\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                metrics = evaluate_performance(target.detach(), pred.detach(), threshold=0.5)\n",
    "\n",
    "                loss_sum += loss.item()\n",
    "                metrics_sum = add_dicts(metrics_sum, metrics)\n",
    "\n",
    "            ## Calculate the average loss and metrics\n",
    "            ## Then log them with MLFlow\n",
    "\n",
    "            avg_loss = loss_sum / len(loader['train'])\n",
    "            avg_metrics = divide_dict(metrics_sum, len(loader['train']))\n",
    "            avg_metrics['loss'] = avg_loss\n",
    "\n",
    "            ## Log the metrics with MLFlow\n",
    "            logger.log_metrics('TRAIN', avg_metrics, epoch)\n",
    "            #logger.save_artifact('.')\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss}')\n",
    "\n",
    "\n",
    "logger = MetricsLogger(columns)\n",
    "train(model, loaders, criterion, optimizer, epochs, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric logging tactics\n",
    "#### When logging metrics only log at every epoch\n",
    "##### - Logger class' sum function sums the output of evaluate functions.\n",
    "##### - Logger class' log function averages and logs the sum function\n",
    "#### Label-wise metrics only logged during testing\n",
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
