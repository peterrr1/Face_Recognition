{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import efficientnet_b0, mobilenet_v2\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch import nn, float32\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('../').resolve()))\n",
    "from utils.transform_utils import DetectFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleNet_V2_X0_5_FaceTransforms(nn.Module):\n",
    "    \"\"\"\n",
    "    A series of transformations to apply to an image before feeding it to a ShuffleNetV2_X0_5 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, detector = None, pad: int = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.detector = detector\n",
    "\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomRotation(degrees=15),\n",
    "            DetectFace(detector, pad),\n",
    "            v2.Resize((224, 224), interpolation=v2.InterpolationMode.BILINEAR),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(dtype=float32, scale=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transforms(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet_B0_FaceTransforms(nn.Module):\n",
    "    \"\"\"\n",
    "    A series of transformations to apply to an image before feeding it to a EfficientNet_B0 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, detector = None, pad: int = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.detector = detector\n",
    "\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomRotation(degrees=15),\n",
    "            DetectFace(detector, pad),\n",
    "            v2.Resize((224, 224), interpolation=v2.InterpolationMode.BICUBIC),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(dtype=float32, scale=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transforms(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from typing import Union, Optional, Callable, Tuple, Any\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "class CelebA(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[str, Path],\n",
    "            transform: Optional[Callable] = None,\n",
    "            \n",
    "    ):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read filenames and attributes\n",
    "        files, attr = self._read_csv('list_attr_celeba.csv')\n",
    "        \n",
    "        self.attr = torch.div(attr + 1, 2, rounding_mode='floor').float()\n",
    "        self.files = [os.path.join(self.root, 'img_align_celeba/', file) for file in files]\n",
    "        \n",
    "\n",
    "    def get_pos_weights(self) -> torch.Tensor:\n",
    "        num_of_labels = len(self.attr)\n",
    "        num_of_pos_labels = torch.sum(self.attr, dim = 0)\n",
    "        num_of_neg_labels = num_of_labels - num_of_pos_labels\n",
    "        pos_weights = num_of_neg_labels / num_of_pos_labels\n",
    "        return pos_weights\n",
    "\n",
    "\n",
    "    def _read_csv(\n",
    "            self,\n",
    "            filename: str,\n",
    "    ) -> Union[torch.Tensor, Tuple]:\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(self.root, filename), index_col=0, header=0)\n",
    "        attr = torch.from_numpy(df.values)\n",
    "        files = df.index.values\n",
    "\n",
    "        return files, attr\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[Any, Any]:\n",
    "        image = Image.open(self.files[index])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.attr[index]\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.attr)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202599"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
